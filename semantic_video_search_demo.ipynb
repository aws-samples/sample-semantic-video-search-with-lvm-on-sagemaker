{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03308c6",
   "metadata": {},
   "source": [
    "# Implementing semantic video search using open-source Large Vision Models on Amazon SageMaker and OpenSearch Serverless\n",
    "\n",
    "---\n",
    "This notebook supplements _[Implementing semantic video search using Large Vision Models on Amazon SageMaker and OpenSearch Serverless](https://link-to-blog)_ blog post and contains code samples for deploying and operating the proposed semantic video search architecture with Large Vision Models on Amazon SageMaker.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Architecture overview](#sec-1)\n",
    "2. [Setup the environment](#sec-2)\n",
    "3. [Setup and create Amazon OpenSearch Serverless (AOSS) collection](#sec-3)\n",
    "4. [Select the Large Vision Model (LVM) and configure indexing parameters](#sec-4)\n",
    "5. [Create an AOSS vector index](#sec-5)\n",
    "6. [Download a few sample videos for testing](#sec-6)\n",
    "7. [Locally embed video and perform semantic text search for testing](#sec-7)\n",
    "8. [Deploy video indexing and video search endpoints with SageMaker](#sec-8)\n",
    "9. [Send videos to SageMaker asyncronous endpoint for indexing](#sec-9)\n",
    "10. [Search AOSS index with text/image queries and visualize results](#sec-10)\n",
    "11. [Clean up resource](#sec-11)\n",
    "  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Architecture overview\n",
    "\n",
    "In this notebook, we demostrate how to deploy and operate the semantic video search architecture (shown below). While the solution is suitable for any open-source Language Vision Model (LVM), the sample code in this demo can leverage an version of **CLIP** or **SigLIP** model families from Hugging Face Model Hub. We leverage **SageMaker Asyncronous Inference** endpoints to embed video frames and ingest them into an **Amazon OpenSearch Serverless** vector index. Using SageMaker asyncronous inference endpoints allows us to handle typically large video payloads and scale resources down to zero, when there are no new videos to index. The vector search collections in Amazon OpenSearch Serverless (AOSS) provides a similarity search capability that is scalable and high performing.\n",
    "\n",
    "<a id='sec-1'></a>\n",
    "\n",
    "![Architecture](doc/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09cd53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Setup the environment\n",
    "\n",
    "<a id='sec-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff89b5a-eccd-47b7-bd7a-a23805dcd9f0",
   "metadata": {},
   "source": [
    "### 2.1 Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b03614",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f56024-91ee-4903-b741-a8ef265b5389",
   "metadata": {},
   "source": [
    "### 2.2 Install Git LFS and pull sample video files (if needed)\n",
    "\n",
    "This repo includes a few sample videos stored remotely as [Git LFS](https://docs.github.com/en/repositories/working-with-files/managing-large-files/collaboration-with-git-large-file-storage) objects. If you did not have Git LFS client installed when pulling this repo, the sample video files might not have been pulled from Git LFS. If this is the case, please install Git LFS and pull video samples by executing the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5187a-b337-4b97-9238-a7a381030848",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# optional: update system packages in Amazon SageMaker Studio Ubuntu environment\n",
    "sudo bash -c 'export DEBIAN_FRONTEND=noninteractive && apt-get -qq update -y && apt-get -qq upgrade -y'\n",
    "\n",
    "# install system packages\n",
    "sudo bash -c 'export DEBIAN_FRONTEND=noninteractive && apt-get -qq install -y git git-lfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8eedd5-6cf3-482e-b4b7-f7d83603f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# pull sample video files from Git LFS\n",
    "git lfs pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647e024",
   "metadata": {},
   "source": [
    "### 2.3 Setup clients and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45e73b-b756-442c-a02d-ef6bef0eb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep the cell outputs clean, let's suppress non-critical logging messages from SageMaker libs\n",
    "import logging\n",
    "logging.getLogger('sagemaker.config').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('sagemaker').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fc84f-0d07-432b-8cc2-1e6b99f19f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Create boto3 session, set AWS region and get account ID\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# Create SageMaker session, setup S3 bucket, and get IAM role\n",
    "bucket_prefix = 'video-search-demo'\n",
    "sm_session = sagemaker.session.Session(boto_session=boto_session, default_bucket_prefix=bucket_prefix)\n",
    "bucket_name = sm_session.default_bucket()\n",
    "iam_role = sagemaker.get_execution_role(sagemaker_session=sm_session)\n",
    "\n",
    "# Define SageMaker instance types for video indexing and search endpoints\n",
    "deploy_instance_type_index = 'ml.g5.2xlarge'\n",
    "deploy_instance_type_search = 'ml.c5.2xlarge'\n",
    "\n",
    "# Define Amazon OpenSearch Serverless client and collection/index names\n",
    "aoss_client = boto3.client('opensearchserverless')\n",
    "aoss_collection_name = 'semantic-video-collection'\n",
    "aoss_index_name = 'frame-index'\n",
    "\n",
    "# Some temporary local paths\n",
    "local_video_dir = 'sample_videos'\n",
    "local_sagemaker_artifact_dir = 'sagemaker_artifact'\n",
    "local_sagemaker_artifact_tarball = 'model.tar.gz'\n",
    "\n",
    "###\n",
    "print(\"AWS Account ID:\", account_id)\n",
    "print(\"AWS Region:\", region)\n",
    "print(\"IAM Role:\", iam_role)\n",
    "print(\"Bucket name:\", bucket_name)\n",
    "print(\"Bucket prefix:\", bucket_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de2130",
   "metadata": {},
   "source": [
    "### 2.4 Attach required permission to IAM role / user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9e97a-fc9b-4b9a-ab8b-ce9f467f5c35",
   "metadata": {},
   "source": [
    "The AWS identity you assume from your notebook environment (which is the [*Studio/notebook Execution Role*](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) from SageMaker, or could be a role or IAM User for self-managed notebooks), must have sufficient [AWS IAM permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html) to read/write from/to your S3 bucket and operate the OpenSearch Serverless service.\n",
    "\n",
    "To grant these permissions to your identity, you can:\n",
    "\n",
    "- Open the [AWS IAM Console](https://us-east-1.console.aws.amazon.com/iam/home?#)\n",
    "- Find your [Role](https://us-east-1.console.aws.amazon.com/iamv2/home?#/roles) (if using SageMaker or otherwise assuming an IAM Role), or else [User](https://us-east-1.console.aws.amazon.com/iamv2/home?#/users)\n",
    "- Select *Add Permissions > Create Inline Policy* to attach the required permissions, open the *JSON* editor and paste in the below example policy:\n",
    "\n",
    "> ⚠️ **Important:** Replace `<bucket_name>` with the name of your Amazon S3 bucket (see printouts of the previous cell).\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:AbortMultipartUpload\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Resource\": \"arn:aws:s3:::<bucket_name>/*\"\n",
    "        },\n",
    "        {\n",
    "          \"Action\": [\n",
    "                \"aoss:CreateCollection\",\n",
    "                \"aoss:ListCollections\",\n",
    "                \"aoss:DeleteCollection\",\n",
    "                \"aoss:CreateAccessPolicy\",\n",
    "                \"aoss:DeleteAccessPolicy\",\n",
    "                \"aoss:CreateSecurityPolicy\",\n",
    "                \"aoss:DeleteSecurityPolicy\",\n",
    "                \"aoss:APIAccessAll\"\n",
    "          ],\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "}\n",
    "```\n",
    "\n",
    "> ⚠️ **Note:** With Amazon SageMaker, your notebook execution role will typically be *separate* from the user or role that you log in to the AWS Console with. If you'd like to explore the OpenSearch Serverless collections/indices from AWS Console, you'll need to grant revelant permissions to your Console user/role too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cac8b2-0487-4301-a43a-088acf72b4ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Setup and create Amazon OpenSearch Serverless (AOSS) collection\n",
    "\n",
    "<a id='sec-3'></a>\n",
    "\n",
    "You need to create policies for the OpenSearch resource. For the overall guidance on OpenSearch security and data access control refer to the corresponding section in the OpenSearch documentation. You need to create a security policy that would enforce encryption, a network policy and access policy. You can use existing methods from our code below for programmatic approach in OpenSearch Serverless or use the AWS console for that.\n",
    "\n",
    "First, we will use boto3-client for AOSS to create encryption, network, and data access policies and associate them with the AOSS collection for vector search, that we will also create later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b660d8a9-e803-4a85-8817-ee8a26c6c85f",
   "metadata": {},
   "source": [
    "### 3.1 Create security components of AOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d5bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encryption policy that matches our AOSS collections name\n",
    "aoss_encryption_policy = aoss_client.create_security_policy(\n",
    "    name=aoss_collection_name + '-ep',\n",
    "    type='encryption',\n",
    "    policy=json.dumps(\n",
    "        {\n",
    "            'Rules': [\n",
    "                {\n",
    "                    'Resource': ['collection/' + aoss_collection_name],\n",
    "                    'ResourceType': 'collection'\n",
    "                }\n",
    "            ],\n",
    "            'AWSOwnedKey': True\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Encryption policy created:')\n",
    "print(aoss_encryption_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad067ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network policy that matches our AOSS collections name\n",
    "aoss_network_policy = aoss_client.create_security_policy(\n",
    "    name=aoss_collection_name + '-np',\n",
    "    type='network',\n",
    "    policy=json.dumps(\n",
    "        [\n",
    "            {\n",
    "                'Rules': [\n",
    "                    {\n",
    "                        'Resource': ['collection/' + aoss_collection_name],\n",
    "                        'ResourceType': 'collection'\n",
    "                    }\n",
    "                ],\n",
    "                'AllowFromPublic': True\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Network policy created:')\n",
    "print(aoss_network_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75e8d5da-7ce9-44e2-99b8-3d3fb02e0ac6",
   "metadata": {},
   "source": [
    "> ⚠️ **Note:** _in order to keep setup overhead at mininum, this proof-of-concept implementation **allows public internet access** to the OpenSearch Serverless collection resource. However, for production environments we strongly suggest to leverage private connection between your VPC and Amazon OpenSearch Serverless resources via an VPC endpoint, as described [here](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-vpc.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6371965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data access policy that matches our AOSS collections name and grants our IAM role access.\n",
    "aoss_data_policy = aoss_client.create_access_policy(\n",
    "    name=aoss_collection_name + '-dp',\n",
    "    type='data',\n",
    "    policy=json.dumps(\n",
    "        [\n",
    "            {\n",
    "                'Rules': [\n",
    "                    {\n",
    "                        'Resource': ['collection/' + aoss_collection_name],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateCollectionItems',\n",
    "                            'aoss:DeleteCollectionItems',\n",
    "                            'aoss:UpdateCollectionItems',\n",
    "                            'aoss:DescribeCollectionItems'],\n",
    "                        'ResourceType': 'collection'\n",
    "                    },\n",
    "                    {\n",
    "                        'Resource': ['index/' + aoss_collection_name + '/*'],\n",
    "                        'Permission': [\n",
    "                            'aoss:CreateIndex',\n",
    "                            'aoss:DeleteIndex',\n",
    "                            'aoss:UpdateIndex',\n",
    "                            'aoss:DescribeIndex',\n",
    "                            'aoss:ReadDocument',\n",
    "                            'aoss:WriteDocument'],\n",
    "                        'ResourceType': 'index'\n",
    "                    }],\n",
    "                'Principal': [iam_role],\n",
    "                'Description': 'My custom easy data policy'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Data policy created:')\n",
    "print(aoss_data_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ad006-22fb-4e7a-91eb-10278f0d7e60",
   "metadata": {},
   "source": [
    "### 3.2 Create AOSS collection for vector seach\n",
    "\n",
    "With all AOSS security components properly set up, we can now create an AOSS collection for vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58adec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request to create collection\n",
    "aoss_collection = aoss_client.create_collection(name=aoss_collection_name, type='VECTORSEARCH')\n",
    "aoss_collection_id = aoss_collection['createCollectionDetail']['id']\n",
    "\n",
    "# Wait until collection becomes active\n",
    "while True:\n",
    "    aoss_reply = aoss_client.list_collections(collectionFilters={'name': aoss_collection_name})\n",
    "    aoss_status = aoss_reply['collectionSummaries'][0]['status']\n",
    "    if aoss_status in ('ACTIVE', 'FAILED'):\n",
    "        print('!')\n",
    "        break\n",
    "    print('-', end='')\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"An AOSS collection created with collection ID:\", aoss_collection_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6155930-3800-45af-8b95-75fbe4883d75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Select the Large Vision Model (LVM) and configure indexing parameters\n",
    "\n",
    "<a id='sec-4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd815a7-ff3e-49d4-8dbf-499d947a88b2",
   "metadata": {},
   "source": [
    "This semantic video search architecture is flexible enough to, in principle, leverage any type of open-source LVMs that can generate text and image embeddings (see, for example, this [list of zero-shot image classification models](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification)). However, the deployable sample code in this demo has been tailored to use either of these two LVM model families available on Hugging Face (HF) Model Hub: **CLIP** (see all models [here](https://huggingface.co/models?library=open_clip)), or **SigLIP** (see all models [here](https://huggingface.co/models?other=siglip)).\n",
    "\n",
    "### 4.1 Inspect the config file\n",
    "Let's choose a concrete model version from **CLIP** or **SigLIP** model families that we want to deploy by specifying its name from HF Model Hub in the `config.yaml` file. Besides specifying the name of a vision model, there are a few other parameters that we'd need to define before deploying the SageMaker endpoints (marked with `<TO_BE_PROVIDED>` placeholders). Let's have a look at the config template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20e2ef-ee78-4aac-ba91-62b414e10330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "\n",
    "config = OmegaConf.load('src/config_template.yaml')\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ddaba-7f25-40c8-83c7-e8c4f9a4bea9",
   "metadata": {},
   "source": [
    "### 4.2 Specify the LVM model name and its embedding dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c5708-f0df-4179-9339-5a2a532a9a75",
   "metadata": {},
   "source": [
    "There are plenty of models to choose from (see, e.g., this list for [CLIP models](https://huggingface.co/models?library=open_clip), or this list for [SigLIP models](https://huggingface.co/models?other=siglip)) and you should typically test a few different models and see how they perform on your particular use-case in terms of search quality and latency requirements. Here are just a few suggestions:\n",
    "\n",
    "- `laion/CLIP-ViT-B-32-laion2B-s34B-b79K` (embedding dimensions: `512`) - one of the *base* OpenCLIP models\n",
    "- `laion/CLIP-ViT-H-14-laion2B-s32B-b79K` (embedding dimensions: `1024`) - one of the *best* OpenCLIP models\n",
    "- `google/siglip-base-patch16-224` (embedding dimensions: `768`) - one of the *base* SigLIP models\n",
    "- `google/siglip-so400m-patch14-384` (embedding dimensions: `1152`) - one of the *best multi-lingual* SigLIP models\n",
    "\n",
    "As a default for this demo, we'll go with the `google/siglip-base-patch16-224` model in order to prioritize indexing and lookup speed over quality. However, SigLIP models tend to outperfrom CLIP models of similar sizes and pre-training procedures (see, e.g., _[Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/pdf/2303.15343.pdf)_ by *Zhai et al.* for more details), so it is fair compromise. You are, of course, welcome to explore other LVMs. \n",
    "\n",
    "> ⚠️ **Important:** When experimenting with other LVMs, **don't forget to edit the model's embedding dimension** in the `config.yaml` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb243376-cd1d-40da-a039-5eed696c32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our vision model and it output dimensions:\n",
    "config.model_name = 'google/siglip-base-patch16-224'\n",
    "config.model_embedding_dim = 768\n",
    "\n",
    "# Specify AOSS resources to be used by SageMaker endpoints:\n",
    "config.aws_region = region\n",
    "config.opensearch.collection_id = aoss_collection_id\n",
    "config.opensearch.index_name = aoss_index_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53051fe-01bc-46a2-acf1-813d37b3341c",
   "metadata": {},
   "source": [
    "Finally, let's save the final config back as `config.yaml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d4936-a6a8-4bdf-96a2-dc2ecbae4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "OmegaConf.save(config, 'src/config.yaml')\n",
    "\n",
    "print(\"Saved the following config:\", \"\\n -------\")\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bfa77-de98-4bb1-bd70-47ec1bba6f92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Create an AOSS vector index\n",
    "\n",
    "<a id='sec-5'></a>\n",
    "\n",
    "Now that we know embedding dimensions of the vectors coming out of selected LVM model, we can create the AOSS index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_aws4auth import AWS4Auth\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "# Use default credential configuration for authentication\n",
    "credentials = boto_session.get_credentials()\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region,\n",
    "    'aoss',\n",
    "    session_token=credentials.token)\n",
    "\n",
    "# Construct AOSS endpoint host\n",
    "host = f\"{aoss_collection_id}.{region}.aoss.amazonaws.com\"\n",
    "\n",
    "# Build the OpenSearch client\n",
    "os_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03593994-db62-426e-83ae-c4326495c842",
   "metadata": {},
   "source": [
    "In our code we provide a method to create a vector database index in OpenSearch Serverless. For our case we chose three parameters for each frame to send to OpenSearch: `timestamp` (time indication in seconds of the picked frame), `video_id` (video name) and `frame_vector` (vector embeddings generated by the chosen model). Feel free to add any further parameters depending on your business use case. \n",
    "\n",
    "In OpenSearch we use [Approximate k-NN (ANN)](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/) search as it can restructure indexes more efficiently compared to Exact k-NN and reduce the dimensionality of searchable vectors. OpenSearch [index space setting](https://opensearch.org/docs/latest/search-plugins/knn/approximate-knn/#spaces) needs to be set to `cosinesimil` when creating an index to enforce search for the nearest vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration for the AOSS vector index\n",
    "index_body = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"timestamp\": {\"type\": \"text\"},\n",
    "            \"video_id\": {\"type\": \"text\"},\n",
    "            \"frame_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": config.model_embedding_dim,\n",
    "                \"method\": {\n",
    "                    \"engine\": \"nmslib\",\n",
    "                    \"space_type\": \"cosinesimil\",\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"parameters\": {\"ef_construction\": config.model_embedding_dim, \"m\": 16}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"number_of_shards\": 2,\n",
    "            \"knn.algo_param\": {\"ef_search\": config.model_embedding_dim},\n",
    "            \"knn\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create AOSS index\n",
    "response = os_client.indices.create(aoss_index_name, body=index_body)\n",
    "\n",
    "print('A new index created:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02848c1-35d8-46a3-a42c-cdf1d0b4d916",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Download a few sample videos for testing\n",
    "\n",
    "<a id='sec-6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e96fc7-1fc6-4802-b7dd-ddc01b3ec1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our video library for testing\n",
    "video_samples_youtube = {\n",
    "    'fashionshow': 'https://www.youtube.com/watch?v=0Py6W56LMK4',\n",
    "    'formulaone': 'https://www.youtube.com/watch?v=6UsInj7lNOk',\n",
    "    'airplanes': 'https://www.youtube.com/watch?v=mCTb7fcPhqU',\n",
    "    'trucking': 'https://www.youtube.com/watch?v=itY07uKxiYQ'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc84c3b-099d-4272-81b1-51fded950820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "def download_youtube_video(vid_dir, vid_name, vid_url):\n",
    "    os.makedirs(vid_dir, exist_ok=True)\n",
    "    yt = YouTube(vid_url)\n",
    "    yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download(\n",
    "        output_path=vid_dir, filename=vid_name)\n",
    "    return os.path.join(vid_dir, vid_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cecba2-6aa1-4b3f-9249-70fb78c8d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_samples_local = {}\n",
    "\n",
    "# Download, if videos are yet available locally\n",
    "for video_name, video_url in video_samples_youtube.items():\n",
    "    video_file = video_name + '.mp4'\n",
    "    video_path = os.path.join(local_video_dir, video_file)\n",
    "    \n",
    "    if not os.path.isfile(video_path):\n",
    "        video_path = download_youtube_video(\n",
    "            vid_dir=local_video_dir,\n",
    "            vid_name=video_file,\n",
    "            vid_url=video_url\n",
    "        )\n",
    "    \n",
    "    video_samples_local[video_name] = video_path\n",
    "    print(\"Video available:\", video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b33e0e-3c26-41eb-b7bb-c89ecd8cb3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "# Let's take the the last video an review it:\n",
    "Video(video_path, width=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4893757d-b72b-47fb-805b-d9b7fad847e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Locally embed video and perform semantic text search for testing\n",
    "\n",
    "<a id='sec-7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e67688-eb01-4f70-836e-954aaf660813",
   "metadata": {},
   "source": [
    "In order to outline the main concepts behind our LVM-based approach for semantic video search, let's first locally embed a video and illustrate a search procedure with a couple of text prompts. \n",
    "\n",
    "> ⚠️ **Note:** In order to keep code duplication for this local demo as small as possible, we will be directly calling a few functions from the `inference.py` script and a few other helper libraries from our deployable package, that we will later actually host on a SageMaker endpoint. This is also a *good practice* to test parts of any custom inference scripts *locally* before deploying to SageMaker endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd59942-dbfa-4d57-8554-ded6d8ee2fc8",
   "metadata": {},
   "source": [
    "### 7.1 Import inference module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afb868-6578-4e55-934b-8af386cd8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As `inference` module initialize parameters from the accompanying config-file, let's set its local path for testing here\n",
    "os.environ[\"CONFIG_FILE\"] = 'src/config.yaml'\n",
    "\n",
    "# Import our `inference.py` script and a few other helper libraries as Python modules\n",
    "from src import inference, processing_funcs, video_loader\n",
    "\n",
    "# Check that our config-file was read successully in inference module\n",
    "print(inference.CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263b632-a788-460d-bb34-391441d5eb9b",
   "metadata": {},
   "source": [
    "### 7.2 Define LVM models from config\n",
    "\n",
    "Any custom inference script for SageMaker hosting must implement a few handler functions. One of these is the `model_fn` that defines how and where to load the ML model by the model server. Concretely, once we deploy our inference scripts as SageMaker endpoint, the SageMaker PyTorch model server will load our model by invoking a `model_fn` function (see, e.g., [this guide](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#load-a-model) for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fc171-cce9-46cf-b9aa-f70e8c3d9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call `model_fn` to load the text and image preprocessing and encoder functions (as specified in config-file).\n",
    "model = inference.model_fn(model_dir=None)\n",
    "\n",
    "print(\"Model and runtime specification loaded:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e1851-9aaf-4301-8dca-087f49a93718",
   "metadata": {},
   "source": [
    "### 7.2 Embed video locally\n",
    "\n",
    "Now we can embed the video locally by calling the same `embed_and_index_video` function from `inference` module (that would be invoked on SageMaker endpoint to process an incoming video). Note that we set `do_index=False` to disable indexing with AOSS and instead keep all video embedding locally for our tests here. See `inference.py` for other implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2189be3-7147-488c-952a-b51ee8dd510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a video from our video library (defined earlier) to vectorize locally\n",
    "vid_name = 'trucking'\n",
    "\n",
    "# Embed video and keep frame vectors locally\n",
    "with open(video_samples_local[vid_name], 'rb') as vid_bytes:\n",
    "    vid_embs, vid_times, vid_inds = inference.embed_and_index_video(model, vid_bytes.read(), vid_name, do_index=False)\n",
    "\n",
    "print(\"Frame embedding shape:\", vid_embs.shape)\n",
    "print(\"Frame timestamps shape:\", vid_times.shape)\n",
    "print(\"Frame indices shape:\", vid_inds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca8c42-ea64-45ec-9618-71fd7551f1e4",
   "metadata": {},
   "source": [
    "### 7.3 Embed text prompts locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc0e96-4329-4f1a-b0dd-b8d909370ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt keys and embed text prompts\n",
    "prompt_keys = ['dog', 'truck', 'cables', 'sketch on whiteboard', 'red truck']\n",
    "prompt_template = 'a photo of a %s'\n",
    "prompts = [prompt_template % k for k in prompt_keys]\n",
    "\n",
    "text_embs = processing_funcs.get_text_embeddings(\n",
    "    prompts,\n",
    "    text_processor=model.processor_text,\n",
    "    text_model=model.model_text,\n",
    "    device=inference.CFG.device,\n",
    "    return_tensors=True\n",
    ")\n",
    "\n",
    "print(\"Prompt keys:\", prompt_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f28d89-626b-42f7-ad30-e60ab1fdab27",
   "metadata": {},
   "source": [
    "### 7.3 Calculate and plot search signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc3de3-967a-4022-ace3-08480bb6d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate search signal scores\n",
    "scores = (vid_embs @ text_embs.T).cpu().numpy()\n",
    "\n",
    "print(\"Search signal shape:\", scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf1d5c3-de38-4ce5-b951-b40738712e08",
   "metadata": {},
   "source": [
    "#### Plot search signal scores for each prompt key\n",
    "\n",
    "Let's visualize the raw search signal scores for each prompt keys. We'll use `plotly` library for interactive plotting so that you can explore signal scores closely. Note that the scores generated locally and shown on the chart differ from OpenSearch as OpenSearch provides its own search scores and not the k-NN similarity scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c9701-cffa-4cd4-8aa0-2499c3259b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(pd.DataFrame(scores, columns=prompt_keys), title=vid_name)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf598da0-1ebb-4747-9134-068b46e7df28",
   "metadata": {},
   "source": [
    "#### Plot search hits and print found frame samples\n",
    "\n",
    "Let's illustrate a realistic search scenario by visualizing a few search hits for our text prompts. \n",
    "\n",
    "> ⚠️ **Note:** When deployed to a SageMaker endpoint, we will use a similar approach for semantic video search using AOSS vector index. However, there will be a few differences:\n",
    "> - AOSS search results will **not** return *all* video frames, but only the ones with highest scores, according to the defined approx. nearest neighbor (ANN) search parameters;\n",
    "> - we will **not** use `frame_distance` parameter as a scene deduplication step, but will be using frame clustering for this (see `src/search_utils.py` for more details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79119d66-e279-4dcc-9a0e-6f8e60bf6f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few helper functions for plotting\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "def filter_hits(scores, top_k=None, **kwargs):\n",
    "    peaks, properties = scipy.signal.find_peaks(scores, **kwargs)\n",
    "    hit_scores = properties['peak_heights']\n",
    "    _ids = hit_scores.argsort()[::-1]\n",
    "    hits = peaks[_ids]\n",
    "    hit_scores = hit_scores[_ids]\n",
    "    if top_k:\n",
    "        hits = hits[:top_k]\n",
    "        hit_scores = hit_scores[:top_k]\n",
    "    return hits, hit_scores\n",
    "\n",
    "def plot_hits(scores, hit_indices, timestamps, threshold=None, title=''):\n",
    "    _steps = len(scores)\n",
    "    _x_coord = np.arange(_steps)\n",
    "    hit_mask = np.zeros(_steps, dtype=bool)\n",
    "    hit_mask[hit_indices] = True\n",
    "    hit_times = timestamps[hit_indices]\n",
    "    fig, ax = plt.subplots(figsize=(12, 4), dpi=200)\n",
    "    ax.plot(scores)\n",
    "\n",
    "    if threshold:\n",
    "        plt.plot(_x_coord, 0 * _x_coord + threshold)\n",
    "\n",
    "    def forward(x):\n",
    "        return np.exp(20 * x)\n",
    "\n",
    "    def inverse(x):\n",
    "        return np.log(x + 1e-3) / 20\n",
    "    \n",
    "    plt.fill_between(_x_coord, hit_mask, alpha=0.2, color='red')\n",
    "    plt.ylim(ymin=0.0, ymax=scores.max() * 1.1)\n",
    "    plt.xticks(_x_coord[::500], timestamps[:_steps][::500].astype('int'))\n",
    "    ax.set_yscale('function', functions=(forward, inverse))\n",
    "    plt.xlabel('timestamp')\n",
    "    plt.ylabel('score')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_images(images):\n",
    "    n = len(images)\n",
    "    f = plt.figure(figsize=(5*len(images), 5))\n",
    "    for i, img in enumerate(images):\n",
    "        f.add_subplot(1, n, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d425e-0e5c-4811-a55e-0db9f2db2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few search parameters to filter results\n",
    "min_score = 0.09       # - show/filter hits above this score\n",
    "frame_distance = 20     # - show/filter hits that are sufficiently spaced apart\n",
    "top_k = 5              # - show/filter only top K hits\n",
    "\n",
    "for sc, key in zip(scores.T, prompt_keys):\n",
    "    hit_indices, hit_scores = filter_hits(sc, height=min_score, top_k=top_k, distance=frame_distance)\n",
    "    hit_indices_expanded = [inference.CFG.video_decoder.sampling_rate * ix for ix in hit_indices]\n",
    "    frames, stamps = video_loader.get_frames_with_indices_jumping(video_samples_local[vid_name], hit_indices_expanded)\n",
    "    plot_hits(sc, hit_indices, vid_times, threshold=min_score, title=f\"The {len(hit_indices)} hit(s) for '{key}'\")\n",
    "    plot_images(frames)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ee359-0848-461e-a154-87b383610c00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Deploy video indexing and video search endpoints with SageMaker\n",
    "\n",
    "<a id='sec-8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a076ff83-5568-4d12-a95c-8195859fb923",
   "metadata": {},
   "source": [
    "### 8.1 Package and upload SageMaker deployable asset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a60e2-e79d-44d8-9c14-60f669b16c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $local_sagemaker_artifact_dir $local_sagemaker_artifact_tarball\n",
    "\n",
    "echo \"Staging all SageMaker deployable assets in '$1' dir:\"\n",
    "rm -rf $1\n",
    "mkdir -vp $1\n",
    "mkdir -vp $1/code\n",
    "cp -v ./src/* $1/code\n",
    "\n",
    "echo\n",
    "echo \"Packaging SageMaker deployable assets to '$2' tarball:\"\n",
    "rm $2\n",
    "tar cvzf $2 -C $1/ . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276b7c2-3084-4087-984f-40e6662ac365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the deployable tarball package to S3\n",
    "model_artifact = sm_session.upload_data(\n",
    "    'model.tar.gz',\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=f'{bucket_prefix}/model'\n",
    ")\n",
    "\n",
    "print(\"Model artifact:\", model_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a18db-bb90-4715-99e0-40d311a24149",
   "metadata": {},
   "source": [
    "### 8.2 Define SageMaker model package\n",
    "\n",
    "Let's collect all the pieces together and define a SageMaker model configuration that we will later deploy as an endpoint. Since for both video indexing and video search we require to have an identical LVM for consistent embeddings, we will have a single model package for both indexing and search workflows. This helps us to keep our codebase for both workflows [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) and automatically reflect any code changes (e.g. update of LVM) in both indexing and search arms of our architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13682bb-e291-4ce7-8bea-e43ddf83a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "env_dict = {\n",
    "    'TS_MAX_REQUEST_SIZE': '200000000',     # increase the TorchServe default max request size to 200MB\n",
    "    'TS_MAX_RESPONSE_SIZE': '100000000', \n",
    "    'TS_DEFAULT_RESPONSE_TIMEOUT': '1000',\n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1',\n",
    "    'CONFIG_FILE': 'config.yaml'\n",
    "}\n",
    "\n",
    "sm_model_name = f\"lvm-{time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())}\"\n",
    "\n",
    "sm_model = PyTorchModel(\n",
    "    name=sm_model_name,\n",
    "    role=iam_role,\n",
    "    model_data=model_artifact,\n",
    "    framework_version='2.0',\n",
    "    py_version='py310',\n",
    "    entry_point='inference.py',\n",
    "    env=env_dict,\n",
    "    sagemaker_session=sm_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e498555-b6cd-4fff-b3db-ff5a0836304b",
   "metadata": {},
   "source": [
    "### 8.3 Deploy SageMaker asyncronous inference endpoint for video indexing\n",
    "\n",
    "Let's deploy the indexing workflow as SageMaker Asynchronous Inference endpoint, which allow to enqueue incoming requests and are ideal for workloads where both the request payload sizes and inference processing times are large (which is typical for large video payloads). Moreover, unlike SageMaker Real-Time Inference endpoints, with SageMaker Asynchronous Inference endpoints you can scale down the number of instances backing your asyncrounous endpoint down to zero. This allows us to deprovision the compute resources of the asyncronous endpoint when there are no traffic (i.e. no new videos to index) and pay only when payloads (i.e. new videos) arrive.\n",
    "\n",
    "> ⚠️ **Note:** In order to keep this notebook compact, we do not set scaling policies for SageMaker Asyncronous Inference endpoints in this demo, but you can [read more about autoscaling policies for asyncronous endpoints here](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-autoscale.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed1a40-89bc-45d5-864a-0f6f18e54882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.async_inference import AsyncInferenceConfig\n",
    "\n",
    "sm_endpoint_name_index = sm_model_name + '-endpoint-index'\n",
    "\n",
    "async_config = AsyncInferenceConfig(\n",
    "    output_path=f\"s3://{bucket_name}/{bucket_prefix}/output\",\n",
    "    max_concurrent_invocations_per_instance=2,\n",
    ")\n",
    "\n",
    "sm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=deploy_instance_type_index,\n",
    "    endpoint_name=sm_endpoint_name_index,\n",
    "    async_inference_config=async_config\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Deployed endpoint name for video indexing:\", sm_endpoint_name_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ed84e-f56c-42c6-8a4a-3899c358d024",
   "metadata": {},
   "source": [
    "### 8.4 Deploy SageMaker real-time inference endpoint for video search\n",
    "\n",
    "As we typically expect (relatively) frequent search queries and have low latency requirements, we use SageMaker Real-Time Inference endpoints to handle the user queries and return semantic search results. We'll now deploy the same model package (as used for video indexing) to the real-time endpoint to facilitate quick video search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e80fc-1947-40fd-8424-3910aee794e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_endpoint_name_search = sm_model_name + '-endpoint-search'\n",
    "\n",
    "sm_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=deploy_instance_type_search,\n",
    "    endpoint_name=sm_endpoint_name_search\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Deployed endpoint name for video search:\", sm_endpoint_name_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea201bb-a0bc-4b85-8356-02f44b6d991a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Send videos to SageMaker asyncronous endpoint for indexing\n",
    "\n",
    "<a id='sec-9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d5d85-9295-4613-a9e1-7c7383e05223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def upload_video(video_path, bucket, prefix):\n",
    "    return sm_session.upload_data(\n",
    "        video_path,\n",
    "        bucket=bucket,\n",
    "        key_prefix=prefix,\n",
    "        extra_args={\"ContentType\": \"video/mp4\"}\n",
    "    )\n",
    "\n",
    "def get_async_process_output(output_location):\n",
    "    output_url = urllib.parse.urlparse(output_location)\n",
    "    bucket = output_url.netloc\n",
    "    key = output_url.path[1:]\n",
    "    while True:\n",
    "        try:\n",
    "            return sm_session.read_s3_file(bucket=bucket, key_prefix=key)\n",
    "        except ClientError as e:\n",
    "            if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "                print(\"-\", end='')\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "\n",
    "def index_video(predictor_async, video_path, video_name):\n",
    "    # uploading\n",
    "    print('Uploading:', video_path)\n",
    "    video_uri = upload_video(video_path, bucket_name, f'{bucket_prefix}/input')\n",
    "    print('Video uploaded:', video_uri)\n",
    "\n",
    "    # async process\n",
    "    async_response = predictor_async.predict_async(\n",
    "        input_path=video_uri,\n",
    "        initial_args={'CustomAttributes': video_name}\n",
    "    )\n",
    "\n",
    "    output_location = async_response.output_path\n",
    "    print('Indexing...')\n",
    "\n",
    "    # waiting\n",
    "    output = get_async_process_output(output_location)\n",
    "    print('')\n",
    "    print('Done!')\n",
    "    for k, v in json.loads(output).items():\n",
    "        print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d2883-ae3e-4fc3-b90a-cf59accd46cd",
   "metadata": {},
   "source": [
    "All is now set up to send our sample video files from S3 to the SageMaker Asyncronous Endpoint for processing with LVM and indexing to an AOSS vector index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956ae91-8632-48e5-b2f9-42ff8be6c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import Predictor\n",
    "from sagemaker.predictor_async import AsyncPredictor\n",
    "\n",
    "# Create an index endpoint client\n",
    "sm_endpoint_predictor_index = AsyncPredictor(\n",
    "    Predictor(\n",
    "        endpoint_name=sm_endpoint_name_index,\n",
    "        sagemaker_session=sm_session\n",
    "    )\n",
    ")\n",
    "\n",
    "# Send a few sample videos for indexing\n",
    "for video_name, video_path in video_samples_local.items():\n",
    "    print(\"Processing video:\", video_name)\n",
    "    index_video(sm_endpoint_predictor_index, video_path, video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064637a0-b725-4e4c-94fd-af0e8eed73eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Search AOSS index with text/image queries and visualize results\n",
    "\n",
    "<a id='sec-10'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3c969-abbc-4b89-81f9-2a3c8d173630",
   "metadata": {},
   "source": [
    "When using OpenSearch API to look up vectors, you need to set parameters `k` for k-Nearest Neighbors and `search_size`. The `search_size` parameter defines the number of results which will be returned in the response, and the `k` parameter is the number of neighbors the search of each graph will return and impacts the performance of search (e.g. precision and recall). For example, decreasing `k` sacrifices recall, but increases search processing speeds significantly. `k` supports a maximum 10,000 nearest neighbors. For best quality results use `k` which is equal or greater to size when performing search.\n",
    "\n",
    "> ⚠️ **Note:** To test search results _with_ or _without_ temporal smoothing, create different indexes in the same collection and restart video processing after switching smoothing ON or OFF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19cde7a-943a-4a70-887a-d183e3dffa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "\n",
    "def search_index(predictor, image=None, text=None, search_size=10, k=10, time_offset=1, video_name=None):\n",
    "    \"\"\" Search for relevant video segments based on an input image or text query.\n",
    "    \n",
    "        Args:\n",
    "            predictor (obj): An SageMaker SDK object that can make predictions based on the input data.\n",
    "            image (str, optional): Path to the input image file. Defaults to None.\n",
    "            text (str, optional): Input text query. Defaults to None.\n",
    "            search_size (int, optional): Number of video segments to search. Defaults to 10.\n",
    "            k (int, optional): Number of nearest neighbors to consider. Defaults to 10.\n",
    "            time_offset (int, optional): Time offset for video segment search. Defaults to 1.\n",
    "            video_name (str, optional): Name of the video file. Defaults to None.\n",
    "    \n",
    "        Returns:\n",
    "            dict: A dictionary containing the search results.\n",
    "    \"\"\"\n",
    "    if image:\n",
    "        with open(image, 'rb') as f:\n",
    "            image = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "    data = {\n",
    "        \"query\": {\n",
    "            \"image\": image,\n",
    "            \"text\": text\n",
    "        },\n",
    "        \"search_args\": {\n",
    "            \"size\": search_size,\n",
    "            \"k\": k,\n",
    "            \"time_offset\": time_offset,\n",
    "            \"name\": video_name\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    prediction = predictor.predict(data=data)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef09879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "endpoint_predictor_search = Predictor(\n",
    "    endpoint_name=sm_endpoint_name_search, \n",
    "    sagemaker_session=sm_session, \n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188b668-8e98-44cc-8bf9-784147f5b0c0",
   "metadata": {},
   "source": [
    "Now that we have a few videos indexed in our AOSS database, let's do our first natural language query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_index(\n",
    "    predictor=endpoint_predictor_search,\n",
    "    image=None,\n",
    "    text=\"formula one\",\n",
    "    search_size=10, \n",
    "    k=10, \n",
    "    time_offset=1, \n",
    "    video_name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b420d",
   "metadata": {},
   "source": [
    "The output of the search function is a dictionary, where for each of the videos with matches we have a set of **temporal clusters**. Each temporal cluster represents a video segment, where the search query semantically matches the content of the video. The clusters are shorter if the match is based off just one or a few video frames, and longer if, for example, a larger video scene displays exactly what we are looking for (by matching multiple frames across the entire temporal cluster).\n",
    "\n",
    "We need to implement a couple of functions to visualize search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e9f8f-6cf7-45f4-9429-44ade0a428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    if not len(imgs) == rows*cols:\n",
    "        raise ValueError(\"number of images must match the grid size (rows * cols)\")\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def get_indices(t_start, t_end, num_indices):\n",
    "    indices = 30 * np.linspace(t_start, t_end, num=num_indices)\n",
    "    indices = indices.astype('int')\n",
    "    return list(indices)\n",
    "\n",
    "\n",
    "def get_frames_from_timestamps(video_path, t_start, t_end, num_frames):\n",
    "    output_frames = []\n",
    "    output_timestamps = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    indices = np.linspace(t_start, t_end, num_frames)\n",
    "    indices = (indices * fps).astype('int')\n",
    "\n",
    "    while cap.isOpened():\n",
    "        for i in indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            success, frame = cap.read()\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            output_frames.append(Image.fromarray(frame, mode='RGB'))\n",
    "            output_timestamps.append(cap.get(cv2.CAP_PROP_POS_MSEC) / 1000)\n",
    "            \n",
    "        cap.release()\n",
    "        return output_frames, output_timestamps\n",
    "    else:\n",
    "        raise Exception(\"Failed to open video '%s'!..\" % video_path)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_results(results, video_dir=local_video_dir):\n",
    "    all_videos = sorted(glob.glob('sample_videos/*.mp4'))\n",
    "\n",
    "    results_local = {video_samples_local[k]: v for (k, v) in results.items() if k in video_samples_local}\n",
    "    \n",
    "    all_results = []\n",
    "    for k, v_list in results_local.items():\n",
    "        for v in v_list:\n",
    "            all_results.append({\n",
    "                'score': v['score'],\n",
    "                'path': k, \n",
    "                't_start': v['t_start'],\n",
    "                't_end': v['t_end']\n",
    "            })\n",
    "\n",
    "    all_results = sorted(all_results, key=lambda x: x['score'], reverse=True)\n",
    "    # visualization\n",
    "    # return all_results\n",
    "    for x in all_results:\n",
    "        title = f\"{x['path'].split('/')[-1].replace('.mp4', '')}: {x['t_start']-0.5:.0f}s - {x['t_end']+0.5:.0f}s\"\n",
    "        print(title)\n",
    "        frames, _ = get_frames_from_timestamps(x['path'], x['t_start']-0.5, x['t_end']+0.5, 5)\n",
    "\n",
    "        central_frame = frames[len(frames) // 2]\n",
    "        draw = ImageDraw.Draw(central_frame)\n",
    "        width, height = central_frame.size\n",
    "        draw.rectangle([(0, 0), (width, height)], outline='red', width=30)\n",
    "        \n",
    "        image = image_grid(frames, 1, 5)\n",
    "        plt.figure(dpi=180)\n",
    "        plt.imshow(image)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.box(False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd37a4",
   "metadata": {},
   "source": [
    "### A. Text Search Across Videos\n",
    "\n",
    "Semantic search across all videos in AOSS vector index (no keyword filters or search terms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d25d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_index(\n",
    "    predictor=endpoint_predictor_search,\n",
    "    image=None,\n",
    "    text=\"a photo of a dog\",\n",
    "    search_size=10, \n",
    "    k=10, \n",
    "    time_offset=2, \n",
    "    video_name=None\n",
    ")\n",
    "\n",
    "\n",
    "visualize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338c210",
   "metadata": {},
   "source": [
    "### B. Text Search Within a Video\n",
    "\n",
    "Hybrid search example (semantic and keyword search) to query scenes from a specific video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbbfaa-2aaf-42eb-997e-57011636ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_index(\n",
    "    predictor=endpoint_predictor_search,\n",
    "    image=None,\n",
    "    text=\"A photo of a sketch on whiteboard\",\n",
    "    search_size=10, \n",
    "    k=10, \n",
    "    time_offset=1, \n",
    "    video_name='trucking'\n",
    ")\n",
    "visualize_results(results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f3dce",
   "metadata": {},
   "source": [
    "### C. Reverse Image Search\n",
    "\n",
    "Reverse image search to query video segments that are simiar to the provided image sample:\n",
    "\n",
    "<img src=\"sample_images/fashionshow.jpg\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef510b-ae25-4e0b-89e4-50d801ee0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_index(\n",
    "    predictor=endpoint_predictor_search,\n",
    "    image=\"sample_images/fashionshow.jpg\",\n",
    "    text=None,\n",
    "    search_size=10, \n",
    "    k=10, \n",
    "    time_offset=3, \n",
    "    video_name=None\n",
    ")\n",
    "visualize_results(results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87c140-1cf4-42db-860c-8eaf3bcf5b50",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Clean up resources\n",
    "\n",
    "<a id='sec-11'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be2fc9-0737-455e-9ce5-ff9839a0dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up AOSS resources\n",
    "aoss_client.delete_collection(id=aoss_collection_id)\n",
    "aoss_client.delete_access_policy(type=\"data\", name=aoss_data_policy['accessPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"network\", name=aoss_network_policy['securityPolicyDetail']['name'])\n",
    "aoss_client.delete_security_policy(type=\"encryption\", name=aoss_encryption_policy['securityPolicyDetail']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44711497-8ed2-4384-8a25-225a764c6f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up SageMaker resources\n",
    "sm_client = boto3.client('sagemaker')\n",
    "sm_client.delete_model(ModelName=sm_model_name)\n",
    "sm_client.delete_endpoint(EndpointName=sm_endpoint_name_index)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=sm_endpoint_name_index)\n",
    "sm_client.delete_endpoint(EndpointName=sm_endpoint_name_search)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=sm_endpoint_name_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1299a8d-9e38-46dd-8131-d1f4c93f2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up S3 resources\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_bucket = s3_resource.Bucket(bucket_name)\n",
    "s3_bucket.objects.filter(Prefix=bucket_prefix).delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
